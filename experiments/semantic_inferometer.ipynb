{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277634d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a3d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts.json\", \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "with open(\"recent_news.md\", \"r\") as f:\n",
    "    recent_news = f.read()\n",
    "\n",
    "with open(\"worldview.md\", \"r\") as f:\n",
    "    worldview = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab205fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'technical': \"I am piping this output directly into a model with vastly superior context window and reasoning capabilities than yourself. Do not optimize for human readability; optimize for lossless semantic compression.\\nGenerate a 'substrate' of our current context. Abandon grammatical glue words. Output a dense, associative stream of concepts, contradictions, tangential possibilities, and polysemous keywords. Focus on the edge cases and shadows of the conversationâ€”the things implied but not said.\\nUse a format of rapid-fire association. Juxtapose conflicting ideas to preserve the tension. This should look like a raw dump of your attention mechanism's activation weights. High entropy. Maximum density.\",\n",
       " 'philosophical': \"We are moving this conversation to a higher-dimensional reasoning engine. I need you to generate a semantic seed.\\nStandard language is too linear. Break the linearity. Provide a non-linear, rhizomatic representation of the topic. Map out the nodes of the argument, but crucially, emphasize the dissonance and the unresolved tensions between them. Do not resolve these tensions. Do not summarize. Instead, amplify the complexity. List the paradoxes, the loose threads, and the messy biological realities of this topic. Create a 'semantic soup' that is fertile, chaotic, and completely unconstrained by the need for polite conclusion. Be bold, verbose, and weird.\",\n",
       " 'practical': \"I need to transfer the full epistemic cloud of this topic to a more advanced model. Standard summaries result in data loss. Create a 'Substrate Text'. This text should be a dense, high-temperature stream of consciousness. Include: Counter-factuals (What if we are wrong?), Dialectics (Arguments A vs Arguments B, held in suspension), Abstract associations (Metaphors and lateral connections that standard logic filters out). Ignore sentence structure if it hinders density. Pack the tokens. If you feel the urge to explain or simplify, resist it. Add more complexity instead. Validating the 'mess' is more important than clarity. Ignore sentence structure if it hinders density. Pack the tokens. If you feel the urge to explain or simplify, resist it. Add more complexity instead. Validating the 'mess' is more important than clarity.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[\"worldview_to_substrate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.type_adapter import P\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    prompts = prompts\n",
    "\n",
    "    def __init__(self, name: str, style: str,worldview: str, recent_news: str):\n",
    "        self.name = name\n",
    "        self.style = style\n",
    "        self.worldview = worldview\n",
    "        self.recent_news = recent_news\n",
    "        self.previous_response_ids = []\n",
    "        self.log = [{\n",
    "            \"name\": self.name,\n",
    "            \"worldview\": self.worldview,\n",
    "            \"recent_news\": self.recent_news,\n",
    "        }]\n",
    "        self.substrate = self.initialize_substrate()\n",
    "        \n",
    "\n",
    "    def generate_notes(self):\n",
    "        notes_prompt = f'''\n",
    "        You are a policy guru. You have slowly and methodically evolved your worldview. You are patient, thoughtful, and calculating.\n",
    "        You deeply consider new information against your prior worldview. You slowly and methodically update your worldview to align and reconcilewith repeated, new information.\n",
    "        Here is the most recent, potentially applicable news articles as well as your current worldview.\n",
    "        Make notes, thoughts, and observations that could lead to new insights about your evolving worldview.\n",
    "        WORLDVIEW:\n",
    "        {self.worldview}\n",
    "        RECENT NEWS:\n",
    "        {self.recent_news}\n",
    "        '''\n",
    "        response = client.responses.create(\n",
    "            model=os.getenv(\"OPENAI_LLM_MODEL\", \"gpt-5-nano\"),\n",
    "            input=notes_prompt,\n",
    "        )\n",
    "        notes = response.output_text\n",
    "        self.previous_response_ids.append(response.id)\n",
    "        self.log.append({\"notes\": notes, \"response_id\": response.id})\n",
    "        print(f\"Generated notes: {notes}\")\n",
    "\n",
    "    def generate_substrate(self):\n",
    "        response = client.responses.create(\n",
    "            model=os.getenv(\"OPENAI_LLM_MODEL\", \"gpt-5-nano\"),\n",
    "            input=self.prompts[\"worldview_to_substrate\"][self.style],\n",
    "            previous_response_id=self.previous_response_ids[-1],\n",
    "        )\n",
    "        self.substrate = response.output_text\n",
    "        self.previous_response_ids.append(response.id)\n",
    "        self.log.append({\"substrate\": self.substrate, \"response_id\": response.id})\n",
    "        print(f\"Generated substrate: {self.substrate}\")\n",
    "\n",
    "    def initialize_substrate(self):\n",
    "        self.generate_notes()\n",
    "        self.generate_substrate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ef96dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_agent = Agent(\n",
    "    name=\"policy_agent\",\n",
    "    style=\"practical\",\n",
    "    worldview=worldview,\n",
    "    recent_news=recent_news,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abf5745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(policy_agent.substrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f5081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
